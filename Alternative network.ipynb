{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a continuation from the `Tabular Keras` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, BatchNormalization, Dropout, Input, LeakyReLU, PReLU\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebalance(frame, col='hab_lbl', factor=1):\n",
    "    max_size = frame[col].value_counts().max()\n",
    "    lst = [frame]\n",
    "    for class_index, group in frame.groupby(col):\n",
    "        lst.append(group.sample(int((max_size-len(group)) / factor), replace=True))\n",
    "    frame_new = pd.concat(lst)\n",
    "    \n",
    "    return frame_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset-rocky-no-STemp.csv')\n",
    "df = df.drop('P. Habitable', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rahul/.local/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(df, train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = rebalance(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3501, 42)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_df['hab_lbl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test_df['hab_lbl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rahul/.local/lib/python3.5/site-packages/pandas/core/frame.py:3940: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "train_df.drop('hab_lbl', axis=1, inplace=True)\n",
    "test_df.drop('hab_lbl', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(np.array(y))\n",
    "y_test = to_categorical(np.array(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(train_df)\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(41,))\n",
    "\n",
    "bn1 = BatchNormalization(name='first_bn')(inp)\n",
    "relu = Dense(5, activation='relu', name='dense1')(bn1)\n",
    "drop1 = Dropout(0.2, name='dropout1')(relu)\n",
    "\n",
    "bn = BatchNormalization(name='bn1')(drop1)\n",
    "relu = Dense(5, activation='relu', name='dense2')(bn)\n",
    "drop2 = Dropout(0.2)(relu)\n",
    "\n",
    "interm = keras.layers.Concatenate()([drop1, drop2])\n",
    "\n",
    "bn = BatchNormalization(name='bn2')(interm)\n",
    "relu = Dense(5, activation='relu', name='dense3')(bn)\n",
    "drop = Dropout(0.2)(relu)\n",
    "\n",
    "interm = keras.layers.Concatenate()([drop, drop2])\n",
    "\n",
    "bn = BatchNormalization()(interm)\n",
    "out = Dense(3, activation='softmax', name='dense4')(bn)\n",
    "\n",
    "model = Model(inputs=inp, outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = K.function([model.layers[0].input], [model.layers[-2].output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3501, 41), (3501, 3))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "\n",
    "    Kz = 0.\n",
    "    for i in range((len(x_train) - 1) // batch_size + 1):\n",
    "        start_i = i * batch_size\n",
    "        end_i = start_i + batch_size\n",
    "        xb = x_train[start_i:end_i]\n",
    "    \n",
    "        activ = np.linalg.norm(func([xb]))\n",
    "        if activ > Kz:\n",
    "            Kz = activ\n",
    "\n",
    "    K_ = (2. * Kz) / (3. * batch_size)\n",
    "    lr = 1 / K_\n",
    "    lrs.append(lr)\n",
    "    print('Epoch', epoch, 'LR =', lr)\n",
    "    return lr\n",
    "    #return K_ / np.sqrt(epoch + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = LearningRateScheduler(lr_schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with standard LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(SGD(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3501 samples, validate on 514 samples\n",
      "Epoch 1/10\n",
      "3501/3501 [==============================] - 3s 911us/step - loss: 0.9727 - acc: 0.5376 - val_loss: 0.2528 - val_acc: 0.9280\n",
      "Epoch 2/10\n",
      "3501/3501 [==============================] - 1s 216us/step - loss: 0.6933 - acc: 0.7241 - val_loss: 0.1950 - val_acc: 0.9397\n",
      "Epoch 3/10\n",
      "3501/3501 [==============================] - 1s 198us/step - loss: 0.5497 - acc: 0.7961 - val_loss: 0.1553 - val_acc: 0.9475\n",
      "Epoch 4/10\n",
      "3501/3501 [==============================] - 1s 204us/step - loss: 0.5031 - acc: 0.8129 - val_loss: 0.1364 - val_acc: 0.9494\n",
      "Epoch 5/10\n",
      "3501/3501 [==============================] - 1s 236us/step - loss: 0.4604 - acc: 0.8218 - val_loss: 0.1202 - val_acc: 0.9553\n",
      "Epoch 6/10\n",
      "3501/3501 [==============================] - 1s 232us/step - loss: 0.4483 - acc: 0.8175 - val_loss: 0.1044 - val_acc: 0.9591\n",
      "Epoch 7/10\n",
      "3501/3501 [==============================] - 1s 213us/step - loss: 0.4314 - acc: 0.8175 - val_loss: 0.1011 - val_acc: 0.9591\n",
      "Epoch 8/10\n",
      "3501/3501 [==============================] - 1s 213us/step - loss: 0.3860 - acc: 0.8475 - val_loss: 0.0895 - val_acc: 0.9650\n",
      "Epoch 9/10\n",
      "3501/3501 [==============================] - 1s 220us/step - loss: 0.3843 - acc: 0.8486 - val_loss: 0.0783 - val_acc: 0.9650\n",
      "Epoch 10/10\n",
      "3501/3501 [==============================] - 1s 239us/step - loss: 0.3602 - acc: 0.8626 - val_loss: 0.0703 - val_acc: 0.9728\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efba75ae4a8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y, validation_data=(np.array(test_df), y_test), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LipschitzLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(SGD(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3501 samples, validate on 514 samples\n",
      "Epoch 1/10\n",
      "Epoch 0 LR = 1.6644086608850448\n",
      "3501/3501 [==============================] - 4s 1ms/step - loss: 0.3586 - acc: 0.8535 - val_loss: 0.0870 - val_acc: 0.9844\n",
      "Epoch 2/10\n",
      "Epoch 1 LR = 0.8434567395433223\n",
      "3501/3501 [==============================] - 1s 242us/step - loss: 0.2088 - acc: 0.9132 - val_loss: 0.0360 - val_acc: 0.9922\n",
      "Epoch 3/10\n",
      "Epoch 2 LR = 1.4378324789465708\n",
      "3501/3501 [==============================] - 1s 262us/step - loss: 0.2072 - acc: 0.9077 - val_loss: 0.0377 - val_acc: 0.9922\n",
      "Epoch 4/10\n",
      "Epoch 3 LR = 1.2105283512667488\n",
      "3501/3501 [==============================] - 1s 268us/step - loss: 0.2155 - acc: 0.9069 - val_loss: 0.0311 - val_acc: 0.9903\n",
      "Epoch 5/10\n",
      "Epoch 4 LR = 1.2339882941421874\n",
      "3501/3501 [==============================] - 1s 246us/step - loss: 0.2027 - acc: 0.9057 - val_loss: 0.0447 - val_acc: 0.9864\n",
      "Epoch 6/10\n",
      "Epoch 5 LR = 0.7318742509912914\n",
      "3501/3501 [==============================] - 1s 248us/step - loss: 0.1699 - acc: 0.9166 - val_loss: 0.0429 - val_acc: 0.9922\n",
      "Epoch 7/10\n",
      "Epoch 6 LR = 0.8234751549008632\n",
      "3501/3501 [==============================] - 1s 268us/step - loss: 0.1692 - acc: 0.9177 - val_loss: 0.0355 - val_acc: 0.9922\n",
      "Epoch 8/10\n",
      "Epoch 7 LR = 0.6448312491006224\n",
      "3501/3501 [==============================] - 1s 261us/step - loss: 0.1577 - acc: 0.9292 - val_loss: 0.0502 - val_acc: 0.9903\n",
      "Epoch 9/10\n",
      "Epoch 8 LR = 0.6774022456986298\n",
      "3501/3501 [==============================] - 1s 246us/step - loss: 0.1674 - acc: 0.9200 - val_loss: 0.0461 - val_acc: 0.9922\n",
      "Epoch 10/10\n",
      "Epoch 9 LR = 0.7037477922086639\n",
      "3501/3501 [==============================] - 1s 266us/step - loss: 0.1849 - acc: 0.9097 - val_loss: 0.0498 - val_acc: 0.9903\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efba4635ef0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y, validation_data=(np.array(test_df), y_test), epochs=10, callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leaky ReLU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(41,))\n",
    "\n",
    "bn1 = BatchNormalization(name='first_bn')(inp)\n",
    "relu = Dense(5, name='dense1')(bn1)\n",
    "relu = LeakyReLU(0.1)(relu)\n",
    "drop1 = Dropout(0.2, name='dropout1')(relu)\n",
    "\n",
    "bn = BatchNormalization(name='bn1')(drop1)\n",
    "relu = Dense(5, activation='relu', name='dense2')(bn)\n",
    "relu = LeakyReLU(0.1)(relu)\n",
    "drop2 = Dropout(0.2)(relu)\n",
    "\n",
    "interm = keras.layers.Concatenate()([drop1, drop2])\n",
    "\n",
    "bn = BatchNormalization(name='bn2')(interm)\n",
    "relu = Dense(5, activation='relu', name='dense3')(bn)\n",
    "relu = LeakyReLU(0.1)(relu)\n",
    "drop = Dropout(0.2)(relu)\n",
    "\n",
    "interm = keras.layers.Concatenate()([drop, drop2])\n",
    "\n",
    "bn = BatchNormalization()(interm)\n",
    "out = Dense(3, activation='softmax', name='dense4')(bn)\n",
    "\n",
    "model = Model(inputs=inp, outputs=out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(SGD(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3501 samples, validate on 514 samples\n",
      "Epoch 1/10\n",
      "3501/3501 [==============================] - 4s 1ms/step - loss: 1.0624 - acc: 0.4670 - val_loss: 0.4723 - val_acc: 0.9708\n",
      "Epoch 2/10\n",
      "3501/3501 [==============================] - 1s 246us/step - loss: 0.7484 - acc: 0.6455 - val_loss: 0.3128 - val_acc: 0.9669\n",
      "Epoch 3/10\n",
      "3501/3501 [==============================] - 1s 218us/step - loss: 0.6174 - acc: 0.7278 - val_loss: 0.2164 - val_acc: 0.9728\n",
      "Epoch 4/10\n",
      "3501/3501 [==============================] - 1s 238us/step - loss: 0.5520 - acc: 0.7504 - val_loss: 0.1753 - val_acc: 0.9689\n",
      "Epoch 5/10\n",
      "3501/3501 [==============================] - 1s 230us/step - loss: 0.5045 - acc: 0.7772 - val_loss: 0.1567 - val_acc: 0.9689\n",
      "Epoch 6/10\n",
      "3501/3501 [==============================] - 1s 215us/step - loss: 0.4511 - acc: 0.8072 - val_loss: 0.1443 - val_acc: 0.9767\n",
      "Epoch 7/10\n",
      "3501/3501 [==============================] - 1s 219us/step - loss: 0.4208 - acc: 0.8252 - val_loss: 0.1219 - val_acc: 0.9747\n",
      "Epoch 8/10\n",
      "3501/3501 [==============================] - 1s 237us/step - loss: 0.3761 - acc: 0.8532 - val_loss: 0.1263 - val_acc: 0.9728\n",
      "Epoch 9/10\n",
      "3501/3501 [==============================] - 1s 235us/step - loss: 0.3334 - acc: 0.8738 - val_loss: 0.1491 - val_acc: 0.9689\n",
      "Epoch 10/10\n",
      "3501/3501 [==============================] - 1s 222us/step - loss: 0.3183 - acc: 0.8769 - val_loss: 0.1516 - val_acc: 0.9708\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efb857a8978>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y, validation_data=(np.array(test_df), y_test), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LipschitzLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(SGD(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3501 samples, validate on 514 samples\n",
      "Epoch 1/10\n",
      "Epoch 0 LR = 0.8341616338518589\n",
      "3501/3501 [==============================] - 5s 1ms/step - loss: 0.3021 - acc: 0.8732 - val_loss: 0.1181 - val_acc: 0.9864\n",
      "Epoch 2/10\n",
      "Epoch 1 LR = 0.8341616338518589\n",
      "3501/3501 [==============================] - 1s 253us/step - loss: 0.1547 - acc: 0.9400 - val_loss: 0.0658 - val_acc: 0.9903\n",
      "Epoch 3/10\n",
      "Epoch 2 LR = 0.8341616338518589\n",
      "3501/3501 [==============================] - 1s 283us/step - loss: 0.1210 - acc: 0.9532 - val_loss: 0.0982 - val_acc: 0.9883\n",
      "Epoch 4/10\n",
      "Epoch 3 LR = 0.8341616338518589\n",
      "3501/3501 [==============================] - 1s 263us/step - loss: 0.1134 - acc: 0.9660 - val_loss: 0.0886 - val_acc: 0.9883\n",
      "Epoch 5/10\n",
      "Epoch 4 LR = 0.8341616338518589\n",
      "3501/3501 [==============================] - 1s 272us/step - loss: 0.1151 - acc: 0.9572 - val_loss: 0.0518 - val_acc: 0.9903\n",
      "Epoch 6/10\n",
      "Epoch 5 LR = 0.8341616338518589\n",
      "3501/3501 [==============================] - 1s 294us/step - loss: 0.0787 - acc: 0.9740 - val_loss: 0.0654 - val_acc: 0.9903\n",
      "Epoch 7/10\n",
      "Epoch 6 LR = 0.8341616338518589\n",
      "3501/3501 [==============================] - 1s 270us/step - loss: 0.0966 - acc: 0.9672 - val_loss: 0.0682 - val_acc: 0.9922\n",
      "Epoch 8/10\n",
      "Epoch 7 LR = 0.8341616338518589\n",
      "3501/3501 [==============================] - 1s 277us/step - loss: 0.0625 - acc: 0.9780 - val_loss: 0.0594 - val_acc: 0.9903\n",
      "Epoch 9/10\n",
      "Epoch 8 LR = 0.8341616338518589\n",
      "3501/3501 [==============================] - 1s 291us/step - loss: 0.0794 - acc: 0.9726 - val_loss: 0.0640 - val_acc: 0.9922\n",
      "Epoch 10/10\n",
      "Epoch 9 LR = 0.8341616338518589\n",
      "3501/3501 [==============================] - 1s 292us/step - loss: 0.0609 - acc: 0.9757 - val_loss: 0.0772 - val_acc: 0.9922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efb84b3dbe0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y, validation_data=(np.array(test_df), y_test), epochs=10, callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(41,))\n",
    "\n",
    "bn1 = BatchNormalization(name='first_bn')(inp)\n",
    "relu = Dense(5, name='dense1')(bn1)\n",
    "relu = PReLU()(relu)\n",
    "drop1 = Dropout(0.2, name='dropout1')(relu)\n",
    "\n",
    "bn = BatchNormalization(name='bn1')(drop1)\n",
    "relu = Dense(5, activation='relu', name='dense2')(bn)\n",
    "relu = PReLU()(relu)\n",
    "drop2 = Dropout(0.2)(relu)\n",
    "\n",
    "interm = keras.layers.Concatenate()([drop1, drop2])\n",
    "\n",
    "bn = BatchNormalization(name='bn2')(interm)\n",
    "relu = Dense(5, activation='relu', name='dense3')(bn)\n",
    "relu = PReLU()(relu)\n",
    "drop = Dropout(0.2)(relu)\n",
    "\n",
    "interm = keras.layers.Concatenate()([drop, drop2])\n",
    "\n",
    "bn = BatchNormalization()(interm)\n",
    "out = Dense(3, activation='softmax', name='dense4')(bn)\n",
    "\n",
    "model = Model(inputs=inp, outputs=out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(SGD(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3501 samples, validate on 514 samples\n",
      "Epoch 1/10\n",
      "3501/3501 [==============================] - 5s 1ms/step - loss: 0.9041 - acc: 0.5990 - val_loss: 0.4179 - val_acc: 0.9183\n",
      "Epoch 2/10\n",
      "3501/3501 [==============================] - 1s 219us/step - loss: 0.6454 - acc: 0.7404 - val_loss: 0.3281 - val_acc: 0.9339\n",
      "Epoch 3/10\n",
      "3501/3501 [==============================] - 1s 221us/step - loss: 0.5647 - acc: 0.7689 - val_loss: 0.2953 - val_acc: 0.9475\n",
      "Epoch 4/10\n",
      "3501/3501 [==============================] - 1s 241us/step - loss: 0.4956 - acc: 0.7963 - val_loss: 0.2709 - val_acc: 0.9591\n",
      "Epoch 5/10\n",
      "3501/3501 [==============================] - 1s 233us/step - loss: 0.4422 - acc: 0.8229 - val_loss: 0.2487 - val_acc: 0.9650\n",
      "Epoch 6/10\n",
      "3501/3501 [==============================] - 1s 220us/step - loss: 0.3946 - acc: 0.8486 - val_loss: 0.2484 - val_acc: 0.9650\n",
      "Epoch 7/10\n",
      "3501/3501 [==============================] - 1s 247us/step - loss: 0.3541 - acc: 0.8638 - val_loss: 0.2393 - val_acc: 0.9669\n",
      "Epoch 8/10\n",
      "3501/3501 [==============================] - 1s 234us/step - loss: 0.3094 - acc: 0.8886 - val_loss: 0.2220 - val_acc: 0.9708\n",
      "Epoch 9/10\n",
      "3501/3501 [==============================] - 1s 235us/step - loss: 0.2876 - acc: 0.8946 - val_loss: 0.2232 - val_acc: 0.9708\n",
      "Epoch 10/10\n",
      "3501/3501 [==============================] - 1s 244us/step - loss: 0.2680 - acc: 0.9069 - val_loss: 0.2085 - val_acc: 0.9708\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efb842d6240>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y, validation_data=(np.array(test_df), y_test), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LipschitzLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(SGD(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3501 samples, validate on 514 samples\n",
      "Epoch 1/10\n",
      "Epoch 0 LR = 0.8341616338518589\n",
      "3501/3501 [==============================] - 6s 2ms/step - loss: 0.2391 - acc: 0.9026 - val_loss: 0.2411 - val_acc: 0.9047\n",
      "Epoch 2/10\n",
      "Epoch 1 LR = 0.8341616338518589\n",
      "3501/3501 [==============================] - 1s 251us/step - loss: 0.1439 - acc: 0.9452 - val_loss: 0.0636 - val_acc: 0.9922\n",
      "Epoch 3/10\n",
      "Epoch 2 LR = 0.8341616338518589\n",
      "3501/3501 [==============================] - 1s 284us/step - loss: 0.1088 - acc: 0.9583 - val_loss: 0.0709 - val_acc: 0.9883\n",
      "Epoch 4/10\n",
      "Epoch 3 LR = 0.8341616338518589\n",
      "3501/3501 [==============================] - 1s 271us/step - loss: 0.0956 - acc: 0.9646 - val_loss: 0.0727 - val_acc: 0.9883\n",
      "Epoch 5/10\n",
      "Epoch 4 LR = 0.8341616338518589\n",
      "3501/3501 [==============================] - 1s 292us/step - loss: 0.1035 - acc: 0.9629 - val_loss: 0.0388 - val_acc: 0.9922\n",
      "Epoch 6/10\n",
      "Epoch 5 LR = 0.8341616338518589\n",
      "3501/3501 [==============================] - 1s 295us/step - loss: 0.0851 - acc: 0.9689 - val_loss: 0.0872 - val_acc: 0.9883\n",
      "Epoch 7/10\n",
      "Epoch 6 LR = 0.8341616338518589\n",
      "3501/3501 [==============================] - 1s 275us/step - loss: 0.0640 - acc: 0.9754 - val_loss: 0.0402 - val_acc: 0.9922\n",
      "Epoch 8/10\n",
      "Epoch 7 LR = 0.8341616338518589\n",
      "3501/3501 [==============================] - 1s 308us/step - loss: 0.0777 - acc: 0.9712 - val_loss: 0.0715 - val_acc: 0.9883\n",
      "Epoch 9/10\n",
      "Epoch 8 LR = 0.8341616338518589\n",
      "3501/3501 [==============================] - 1s 284us/step - loss: 0.0568 - acc: 0.9777 - val_loss: 0.0753 - val_acc: 0.9922\n",
      "Epoch 10/10\n",
      "Epoch 9 LR = 0.8341616338518589\n",
      "3501/3501 [==============================] - 1s 283us/step - loss: 0.0641 - acc: 0.9780 - val_loss: 0.1308 - val_acc: 0.9825\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efb775eeba8>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y, validation_data=(np.array(test_df), y_test), epochs=10, callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset with no surface temp related features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset_no_ST_related_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rahul/.local/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n",
      "/home/rahul/.local/lib/python3.5/site-packages/pandas/core/frame.py:3940: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(df, train_size=0.7)\n",
    "train_df = rebalance(train_df)\n",
    "\n",
    "y = train_df['hab_lbl']\n",
    "y_test = test_df['hab_lbl']\n",
    "\n",
    "train_df.drop('hab_lbl', axis=1, inplace=True)\n",
    "test_df.drop('hab_lbl', axis=1, inplace=True)\n",
    "\n",
    "y = to_categorical(np.array(y))\n",
    "y_test = to_categorical(np.array(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(9,))\n",
    "\n",
    "bn1 = BatchNormalization(name='first_bn')(inp)\n",
    "relu = Dense(5, activation='relu', name='dense1')(bn1)\n",
    "drop1 = Dropout(0.2, name='dropout1')(relu)\n",
    "\n",
    "bn = BatchNormalization(name='bn1')(drop1)\n",
    "relu = Dense(5, activation='relu', name='dense2')(bn)\n",
    "drop2 = Dropout(0.2)(relu)\n",
    "\n",
    "interm = keras.layers.Concatenate()([drop1, drop2])\n",
    "\n",
    "bn = BatchNormalization(name='bn2')(interm)\n",
    "relu = Dense(5, activation='relu', name='dense3')(bn)\n",
    "drop = Dropout(0.2)(relu)\n",
    "\n",
    "interm = keras.layers.Concatenate()([drop, drop2])\n",
    "\n",
    "bn = BatchNormalization()(interm)\n",
    "out = Dense(3, activation='softmax', name='dense4')(bn)\n",
    "\n",
    "model = Model(inputs=inp, outputs=out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = []\n",
    "model.compile(SGD(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7926 samples, validate on 1149 samples\n",
      "Epoch 1/10\n",
      "7926/7926 [==============================] - 6s 780us/step - loss: 0.8853 - acc: 0.5193 - val_loss: 0.6336 - val_acc: 0.7946\n",
      "Epoch 2/10\n",
      "7926/7926 [==============================] - 2s 208us/step - loss: 0.7402 - acc: 0.6088 - val_loss: 0.5351 - val_acc: 0.8338\n",
      "Epoch 3/10\n",
      "7926/7926 [==============================] - 2s 219us/step - loss: 0.6983 - acc: 0.6292 - val_loss: 0.3932 - val_acc: 0.8947\n",
      "Epoch 4/10\n",
      "7926/7926 [==============================] - 2s 205us/step - loss: 0.6563 - acc: 0.6411 - val_loss: 0.3311 - val_acc: 0.9130\n",
      "Epoch 5/10\n",
      "7926/7926 [==============================] - 2s 210us/step - loss: 0.6406 - acc: 0.6466 - val_loss: 0.2807 - val_acc: 0.9269\n",
      "Epoch 6/10\n",
      "7926/7926 [==============================] - 2s 215us/step - loss: 0.6289 - acc: 0.6511 - val_loss: 0.2593 - val_acc: 0.9295\n",
      "Epoch 7/10\n",
      "7926/7926 [==============================] - 2s 214us/step - loss: 0.6177 - acc: 0.6518 - val_loss: 0.2492 - val_acc: 0.9304\n",
      "Epoch 8/10\n",
      "7926/7926 [==============================] - 2s 231us/step - loss: 0.6044 - acc: 0.6575 - val_loss: 0.2290 - val_acc: 0.9339\n",
      "Epoch 9/10\n",
      "7926/7926 [==============================] - 2s 223us/step - loss: 0.5977 - acc: 0.6639 - val_loss: 0.2105 - val_acc: 0.9339\n",
      "Epoch 10/10\n",
      "7926/7926 [==============================] - 2s 218us/step - loss: 0.5877 - acc: 0.6707 - val_loss: 0.2033 - val_acc: 0.9373\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efb75448fd0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = np.array(train_df)\n",
    "batch_size = 32\n",
    "\n",
    "model.fit(x_train, y, validation_data=(np.array(test_df), y_test), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LipschitzLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(SGD(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = K.function([model.layers[0].input], [model.layers[-2].output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7926 samples, validate on 1149 samples\n",
      "Epoch 1/10\n",
      "Epoch 0 LR = 1.2442820769520127\n",
      "7926/7926 [==============================] - 7s 836us/step - loss: 0.6275 - acc: 0.6474 - val_loss: 0.2005 - val_acc: 0.9347\n",
      "Epoch 2/10\n",
      "Epoch 1 LR = 0.6892385140170064\n",
      "7926/7926 [==============================] - 2s 279us/step - loss: 0.5721 - acc: 0.6747 - val_loss: 0.1350 - val_acc: 0.9556\n",
      "Epoch 3/10\n",
      "Epoch 2 LR = 0.9017260115411417\n",
      "7926/7926 [==============================] - 2s 292us/step - loss: 0.5946 - acc: 0.6648 - val_loss: 0.4779 - val_acc: 0.9077\n",
      "Epoch 4/10\n",
      "Epoch 3 LR = 0.7794945472479028\n",
      "7926/7926 [==============================] - 2s 265us/step - loss: 0.5799 - acc: 0.6842 - val_loss: 0.2903 - val_acc: 0.9225\n",
      "Epoch 5/10\n",
      "Epoch 4 LR = 0.7414202820021162\n",
      "7926/7926 [==============================] - 2s 275us/step - loss: 0.5872 - acc: 0.6877 - val_loss: 0.3124 - val_acc: 0.9199\n",
      "Epoch 6/10\n",
      "Epoch 5 LR = 1.0307085808584373\n",
      "7926/7926 [==============================] - 2s 264us/step - loss: 0.5964 - acc: 0.6858 - val_loss: 0.2971 - val_acc: 0.9225\n",
      "Epoch 7/10\n",
      "Epoch 6 LR = 0.9613011507962247\n",
      "7926/7926 [==============================] - 2s 276us/step - loss: 0.5701 - acc: 0.6978 - val_loss: 0.1870 - val_acc: 0.9452\n",
      "Epoch 8/10\n",
      "Epoch 7 LR = 0.6143124724709634\n",
      "7926/7926 [==============================] - 2s 279us/step - loss: 0.5721 - acc: 0.7009 - val_loss: 0.1624 - val_acc: 0.9426\n",
      "Epoch 9/10\n",
      "Epoch 8 LR = 0.6965073314510215\n",
      "7926/7926 [==============================] - 2s 280us/step - loss: 0.5723 - acc: 0.7050 - val_loss: 0.1741 - val_acc: 0.9417\n",
      "Epoch 10/10\n",
      "Epoch 9 LR = 0.7778736878351215\n",
      "7926/7926 [==============================] - 2s 280us/step - loss: 0.5660 - acc: 0.7033 - val_loss: 0.1094 - val_acc: 0.9643\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efb7443f630>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y, validation_data=(np.array(test_df), y_test), epochs=10, callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(9,))\n",
    "\n",
    "bn1 = BatchNormalization(name='first_bn')(inp)\n",
    "relu = Dense(5, name='dense1')(bn1)\n",
    "relu = LeakyReLU(0.1)(relu)\n",
    "drop1 = Dropout(0.2, name='dropout1')(relu)\n",
    "\n",
    "bn = BatchNormalization(name='bn1')(drop1)\n",
    "relu = Dense(5, activation='relu', name='dense2')(bn)\n",
    "relu = LeakyReLU(0.1)(relu)\n",
    "drop2 = Dropout(0.2)(relu)\n",
    "\n",
    "interm = keras.layers.Concatenate()([drop1, drop2])\n",
    "\n",
    "bn = BatchNormalization(name='bn2')(interm)\n",
    "relu = Dense(5, activation='relu', name='dense3')(bn)\n",
    "relu = LeakyReLU(0.1)(relu)\n",
    "drop = Dropout(0.2)(relu)\n",
    "\n",
    "interm = keras.layers.Concatenate()([drop, drop2])\n",
    "\n",
    "bn = BatchNormalization()(interm)\n",
    "out = Dense(3, activation='softmax', name='dense4')(bn)\n",
    "\n",
    "model = Model(inputs=inp, outputs=out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(SGD(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7926 samples, validate on 1149 samples\n",
      "Epoch 1/10\n",
      "7926/7926 [==============================] - 7s 914us/step - loss: 0.9227 - acc: 0.5453 - val_loss: 0.5229 - val_acc: 0.9504\n",
      "Epoch 2/10\n",
      "7926/7926 [==============================] - 2s 228us/step - loss: 0.7580 - acc: 0.6427 - val_loss: 0.3963 - val_acc: 0.9330\n",
      "Epoch 3/10\n",
      "7926/7926 [==============================] - 2s 225us/step - loss: 0.6939 - acc: 0.6606 - val_loss: 0.3527 - val_acc: 0.9269\n",
      "Epoch 4/10\n",
      "7926/7926 [==============================] - 2s 234us/step - loss: 0.6619 - acc: 0.6764 - val_loss: 0.2903 - val_acc: 0.9295\n",
      "Epoch 5/10\n",
      "7926/7926 [==============================] - 2s 225us/step - loss: 0.6253 - acc: 0.6944 - val_loss: 0.2624 - val_acc: 0.9278\n",
      "Epoch 6/10\n",
      "7926/7926 [==============================] - 2s 234us/step - loss: 0.6252 - acc: 0.6938 - val_loss: 0.2465 - val_acc: 0.9260\n",
      "Epoch 7/10\n",
      "7926/7926 [==============================] - 2s 234us/step - loss: 0.6003 - acc: 0.7082 - val_loss: 0.2358 - val_acc: 0.9286\n",
      "Epoch 8/10\n",
      "7926/7926 [==============================] - 2s 231us/step - loss: 0.5956 - acc: 0.7079 - val_loss: 0.2265 - val_acc: 0.9330\n",
      "Epoch 9/10\n",
      "7926/7926 [==============================] - 2s 236us/step - loss: 0.5882 - acc: 0.7036 - val_loss: 0.2210 - val_acc: 0.9312\n",
      "Epoch 10/10\n",
      "7926/7926 [==============================] - 2s 251us/step - loss: 0.5789 - acc: 0.7088 - val_loss: 0.1963 - val_acc: 0.9382\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efb74427160>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y, validation_data=(np.array(test_df), y_test), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LipschitzLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(SGD(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = K.function([model.layers[0].input], [model.layers[-2].output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7926 samples, validate on 1149 samples\n",
      "Epoch 1/10\n",
      "Epoch 0 LR = 0.9730454548336872\n",
      "7926/7926 [==============================] - 4s 479us/step - loss: 0.5094 - acc: 0.7328 - val_loss: 0.1981 - val_acc: 0.9426\n",
      "Epoch 2/10\n",
      "Epoch 1 LR = 0.828665507286387\n",
      "7926/7926 [==============================] - 2s 282us/step - loss: 0.4994 - acc: 0.7417 - val_loss: 0.2465 - val_acc: 0.9408\n",
      "Epoch 3/10\n",
      "Epoch 2 LR = 0.9383622983780305\n",
      "7926/7926 [==============================] - 2s 288us/step - loss: 0.5318 - acc: 0.7239 - val_loss: 0.0866 - val_acc: 0.9748\n",
      "Epoch 4/10\n",
      "Epoch 3 LR = 0.7186669197981984\n",
      "7926/7926 [==============================] - 2s 284us/step - loss: 0.5221 - acc: 0.7228 - val_loss: 0.3663 - val_acc: 0.9156\n",
      "Epoch 5/10\n",
      "Epoch 4 LR = 0.7572325907050088\n",
      "7926/7926 [==============================] - 2s 281us/step - loss: 0.4950 - acc: 0.7440 - val_loss: 0.1911 - val_acc: 0.9452\n",
      "Epoch 6/10\n",
      "Epoch 5 LR = 0.9331614973227133\n",
      "7926/7926 [==============================] - 2s 283us/step - loss: 0.5152 - acc: 0.7330 - val_loss: 0.1421 - val_acc: 0.9513\n",
      "Epoch 7/10\n",
      "Epoch 6 LR = 0.7854895107732609\n",
      "7926/7926 [==============================] - 2s 285us/step - loss: 0.5007 - acc: 0.7455 - val_loss: 0.2162 - val_acc: 0.9408\n",
      "Epoch 8/10\n",
      "Epoch 7 LR = 1.0498735562260324\n",
      "7926/7926 [==============================] - 2s 285us/step - loss: 0.5253 - acc: 0.7285 - val_loss: 0.2125 - val_acc: 0.9391\n",
      "Epoch 9/10\n",
      "Epoch 8 LR = 0.9842783451930459\n",
      "7926/7926 [==============================] - 2s 292us/step - loss: 0.5107 - acc: 0.7361 - val_loss: 0.1989 - val_acc: 0.9487\n",
      "Epoch 10/10\n",
      "Epoch 9 LR = 0.8469994482988137\n",
      "7926/7926 [==============================] - 2s 305us/step - loss: 0.5054 - acc: 0.7407 - val_loss: 0.1773 - val_acc: 0.9469\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efb76cd9cc0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y, validation_data=(np.array(test_df), y_test), epochs=10, callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(9,))\n",
    "\n",
    "bn1 = BatchNormalization(name='first_bn')(inp)\n",
    "relu = Dense(5, name='dense1')(bn1)\n",
    "relu = PReLU()(relu)\n",
    "drop1 = Dropout(0.2, name='dropout1')(relu)\n",
    "\n",
    "bn = BatchNormalization(name='bn1')(drop1)\n",
    "relu = Dense(5, activation='relu', name='dense2')(bn)\n",
    "relu = PReLU()(relu)\n",
    "drop2 = Dropout(0.2)(relu)\n",
    "\n",
    "interm = keras.layers.Concatenate()([drop1, drop2])\n",
    "\n",
    "bn = BatchNormalization(name='bn2')(interm)\n",
    "relu = Dense(5, activation='relu', name='dense3')(bn)\n",
    "relu = PReLU()(relu)\n",
    "drop = Dropout(0.2)(relu)\n",
    "\n",
    "interm = keras.layers.Concatenate()([drop, drop2])\n",
    "\n",
    "bn = BatchNormalization()(interm)\n",
    "out = Dense(3, activation='softmax', name='dense4')(bn)\n",
    "\n",
    "model = Model(inputs=inp, outputs=out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(SGD(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7926 samples, validate on 1149 samples\n",
      "Epoch 1/10\n",
      "7926/7926 [==============================] - 8s 1ms/step - loss: 0.9009 - acc: 0.5351 - val_loss: 0.3560 - val_acc: 0.9312\n",
      "Epoch 2/10\n",
      "7926/7926 [==============================] - 2s 238us/step - loss: 0.7026 - acc: 0.6342 - val_loss: 0.2939 - val_acc: 0.9382\n",
      "Epoch 3/10\n",
      "7926/7926 [==============================] - 2s 230us/step - loss: 0.6528 - acc: 0.6583 - val_loss: 0.2633 - val_acc: 0.9408\n",
      "Epoch 4/10\n",
      "7926/7926 [==============================] - 2s 226us/step - loss: 0.6228 - acc: 0.6794 - val_loss: 0.2589 - val_acc: 0.9399\n",
      "Epoch 5/10\n",
      "7926/7926 [==============================] - 2s 235us/step - loss: 0.6069 - acc: 0.6827 - val_loss: 0.2458 - val_acc: 0.9399\n",
      "Epoch 6/10\n",
      "7926/7926 [==============================] - 2s 245us/step - loss: 0.5960 - acc: 0.6860 - val_loss: 0.2385 - val_acc: 0.9460\n",
      "Epoch 7/10\n",
      "7926/7926 [==============================] - 2s 251us/step - loss: 0.5928 - acc: 0.6834 - val_loss: 0.2352 - val_acc: 0.9460\n",
      "Epoch 8/10\n",
      "7926/7926 [==============================] - 2s 265us/step - loss: 0.5766 - acc: 0.6939 - val_loss: 0.2285 - val_acc: 0.9478\n",
      "Epoch 9/10\n",
      "7926/7926 [==============================] - 2s 263us/step - loss: 0.5647 - acc: 0.7012 - val_loss: 0.2245 - val_acc: 0.9504\n",
      "Epoch 10/10\n",
      "7926/7926 [==============================] - 2s 284us/step - loss: 0.5838 - acc: 0.6953 - val_loss: 0.2198 - val_acc: 0.9495\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efb744274e0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y, validation_data=(np.array(test_df), y_test), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LipschitzLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(SGD(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = K.function([model.layers[0].input], [model.layers[-2].output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7926 samples, validate on 1149 samples\n",
      "Epoch 1/10\n",
      "Epoch 0 LR = 0.8288305687340369\n",
      "7926/7926 [==============================] - 11s 1ms/step - loss: 0.5947 - acc: 0.6640 - val_loss: 0.1465 - val_acc: 0.9530\n",
      "Epoch 2/10\n",
      "Epoch 1 LR = 0.8016015069168954\n",
      "7926/7926 [==============================] - 2s 314us/step - loss: 0.5623 - acc: 0.6869 - val_loss: 0.1749 - val_acc: 0.9513\n",
      "Epoch 3/10\n",
      "Epoch 2 LR = 0.9804867512194028\n",
      "7926/7926 [==============================] - 3s 324us/step - loss: 0.5673 - acc: 0.6841 - val_loss: 0.1452 - val_acc: 0.9478\n",
      "Epoch 4/10\n",
      "Epoch 3 LR = 0.9209644086449322\n",
      "7926/7926 [==============================] - 3s 340us/step - loss: 0.5524 - acc: 0.6870 - val_loss: 0.1783 - val_acc: 0.9504\n",
      "Epoch 5/10\n",
      "Epoch 4 LR = 0.9022393230382826\n",
      "7926/7926 [==============================] - 3s 333us/step - loss: 0.5388 - acc: 0.7082 - val_loss: 0.1374 - val_acc: 0.9556\n",
      "Epoch 6/10\n",
      "Epoch 5 LR = 0.792674659740621\n",
      "7926/7926 [==============================] - 2s 304us/step - loss: 0.5243 - acc: 0.7197 - val_loss: 0.1607 - val_acc: 0.9504\n",
      "Epoch 7/10\n",
      "Epoch 6 LR = 0.627838189753657\n",
      "7926/7926 [==============================] - 2s 307us/step - loss: 0.5108 - acc: 0.7433 - val_loss: 0.0846 - val_acc: 0.9600\n",
      "Epoch 8/10\n",
      "Epoch 7 LR = 0.7425346508758582\n",
      "7926/7926 [==============================] - 2s 310us/step - loss: 0.5267 - acc: 0.7214 - val_loss: 0.1209 - val_acc: 0.9547\n",
      "Epoch 9/10\n",
      "Epoch 8 LR = 0.8609383589002956\n",
      "7926/7926 [==============================] - 3s 319us/step - loss: 0.5196 - acc: 0.7289 - val_loss: 0.2238 - val_acc: 0.9347\n",
      "Epoch 10/10\n",
      "Epoch 9 LR = 0.8702227579746483\n",
      "7926/7926 [==============================] - 3s 337us/step - loss: 0.5350 - acc: 0.7175 - val_loss: 0.1752 - val_acc: 0.9443\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efb6da274e0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y, validation_data=(np.array(test_df), y_test), epochs=10, callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
